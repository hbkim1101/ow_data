import requests
import pandas as pd
import time
import html
import json
import os
from bs4 import BeautifulSoup
from itertools import product
from datetime import datetime
from urllib.parse import unquote
from concurrent.futures import ThreadPoolExecutor, as_completed

# ===== ì„¤ì •ê°’ =====
MAX_WORKERS = 5  # ë™ì‹œ ìš”ì²­ ìˆ˜ (5~8 ê¶Œì¥)
TIMEOUT_SEC = 30 # íƒ€ì„ì•„ì›ƒ

def scrape_single_url(args):
    """
    [ì‘ì—… ë‹¨ìœ„] URL ìš”ì²­ ë° ì •ë°€ ê²€ì¦(Validation) ìˆ˜í–‰
    """
    region, gamemode, map_name, tier, date_str = args
    
    records = []
    
    # 1. ìš”ì²­ URL ì¡°ë¦½
    base_url = "https://overwatch.blizzard.com/ko-kr/rates/"
    # rq: 0(ë¹ ë¥¸ëŒ€ì „), 2(ê²½ìŸì „)
    params = f"?input=pc&map={map_name}&region={region}&role=All&rq={gamemode}&tier={tier}"
    target_url = base_url + params

    max_retries = 3
    for attempt in range(max_retries):
        try:
            # 2. ìš”ì²­ ì „ì†¡ (allow_redirects=Trueë¡œ ì„¤ì •í•˜ì—¬ ìµœì¢… ë„ì°©ì§€ í™•ì¸)
            res = requests.get(target_url, timeout=TIMEOUT_SEC, allow_redirects=True)
            res.raise_for_status()

            # ===== ğŸ›¡ï¸ [í•µì‹¬] URL ëŒ€ì¡° ê²€ì¦ (Validation) =====
            # ë¸Œë¼ìš°ì €/ì„œë²„ ê°„ ì¸ì½”ë”© ì°¨ì´ í•´ê²°ì„ ìœ„í•´ ë””ì½”ë”©
            final_url_decoded = unquote(res.url)
            
            # (1) ê²Œì„ ëª¨ë“œ(rq) ê²€ì¦
            # ë‚´ê°€ ìš”ì²­í•œ ëª¨ë“œ(rq=2)ê°€ ì‚¬ë¼ì§€ê³  rq=0 ë“±ìœ¼ë¡œ ë°”ë€Œì—ˆëŠ”ì§€ í™•ì¸
            if f"rq={gamemode}" not in final_url_decoded:
                # print(f"â© [SKIP] GameMode Mismatch: {map_name}/{tier}")
                return []

            # (2) ë§µ ì´ë¦„ ê²€ì¦
            if map_name not in final_url_decoded:
                # print(f"â© [SKIP] Map Mismatch: {map_name} -> Removed in URL")
                return []

            # (3) í‹°ì–´ ê²€ì¦
            if tier not in final_url_decoded:
                 # print(f"â© [SKIP] Tier Mismatch: {tier} -> Removed in URL")
                 return []
            
            # ===================================================

            # 3. ë°ì´í„° íŒŒì‹±
            soup = BeautifulSoup(res.text, "html.parser")
            tag = soup.find("blz-data-table")
            
            # ë°ì´í„° í…Œì´ë¸”ì´ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°
            if not tag:
                return []

            raw_json = html.unescape(tag["allrows"])
            data = json.loads(raw_json)

            for hero in data:
                cells = hero.get("cells", {})
                hero_meta = hero.get("hero", {})
                records.append({
                    "date": date_str,
                    "game_mode": "competitive" if gamemode == 1 else "quickplay",
                    "region": region,
                    "map": map_name,
                    "tier": tier,
                    "hero_name": cells.get("name", ""),
                    "role": hero_meta.get("role", ""),
                    "pick_rate(%)": cells.get("pickrate", ""),
                    "win_rate(%)": cells.get("winrate", "")
                })
            
            # ì„±ê³µ ì‹œ ì•½ê°„ì˜ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            time.sleep(0.1) 
            return records

        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(1) # ì¬ì‹œë„ ì „ ëŒ€ê¸°
            else:
                # ì‹¤íŒ¨ ë¡œê·¸ (í•„ìš” ì‹œ ì£¼ì„ í•´ì œ)
                # print(f"âŒ [FAIL] {map_name}/{tier}: {e}")
                return [] 

    return []

def main():
    # ===== 0. ê¸°ë³¸ ì„¤ì • =====
    date_str = datetime.now().strftime("%Y-%m-%d")
    season_dir = "Season20"
    season_num = "".join(ch for ch in season_dir if ch.isdigit())
    season_code = f"S{season_num}"
    date_short = datetime.strptime(date_str, "%Y-%m-%d").strftime("%y%m%d")

    save_root = os.path.join(season_dir, date_str)
    os.makedirs(save_root, exist_ok=True)

    print(f"=== Saving data under: {save_root} ===")
    print(f"=== Workers: {MAX_WORKERS} threads ===")

    # ===== 1. ìˆ˜ì§‘ ëŒ€ìƒ ì„¤ì • =====
    gamemodes = [0, 1]
    regions = ["Asia"]
    maps = [
        "all-maps", "throne-of-anubis", "hanaoka", "antarctic-peninsula", "nepal", "lijiang-tower", 
        "busan", "samoa", "oasis", "ilios", "route-66", "watchpoint-gibraltar", "dorado", 
        "rialto", "shambali-monastery", "circuit-royal", "junkertown", "havana", "new-junk-city", 
        "suravasa", "aatlis", "numbani", "midtown", "blizzard-world", "eichenwalde", 
        "kings-row", "paraiso", "hollywood", "new-queen-street", "runasapi", "esperanca", "colosseo"
    ]
    tiers = ["All", "Bronze", "Silver", "Gold", "Platinum", "Diamond", "Master", "Grandmaster"]

    total_rows = 0

    # ===== 2. ì§€ì—­ë³„ ìˆ˜ì§‘ =====
    for region in regions:
        print(f"\n===== ğŸŒ {region} ìˆ˜ì§‘ ì‹œì‘ (Parallel) =====")
        
        # ì‘ì—… ëª©ë¡(Task List) ìƒì„±
        tasks = []
        for gamemode, map_name, tier in product(gamemodes, maps, tiers):
            # 1ì°¨ í•„í„°ë§ (ë¶ˆí•„ìš”í•œ ì¡°í•© ì œì™¸)
            if gamemode == 0 and tier != "All": continue
            elif gamemode == 1 and map_name in ["throne-of-anubis", "hanaoka"]: continue
            
            tasks.append((region, gamemode, map_name, tier, date_str))

        region_records = []
        
        # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_url = {executor.submit(scrape_single_url, t): t for t in tasks}
            
            for i, future in enumerate(as_completed(future_to_url)):
                try:
                    data = future.result()
                    if data:
                        region_records.extend(data)
                except Exception as exc:
                    print(f"Error in worker: {exc}")
                
                # ì§„í–‰ ìƒí™© ë¡œê¹… (50ê°œ ë‹¨ìœ„)
                if (i + 1) % 50 == 0:
                    print(f"   ... {i + 1}/{len(tasks)} ì™„ë£Œ")

        # ===== 3. ì €ì¥ =====
        if region_records:
            df_region = pd.DataFrame(region_records)
            total_rows += len(df_region)

            filename = f"{season_code}_{region}_{date_short}.csv"
            filepath = os.path.join(save_root, filename)
            df_region.to_csv(filepath, index=False, encoding="utf-8-sig")
            print(f"ğŸ’¾ {region} ì €ì¥ ì™„ë£Œ: {len(df_region)} rows")
        else:
            print(f"âš ï¸ {region} ë°ì´í„° ì—†ìŒ")

    print(f"\nğŸ‰ ì „ì²´ ì™„ë£Œ! ì´ ë°ì´í„° í–‰ ìˆ˜: {total_rows}")

    # GitHub Actions í™˜ê²½ ë³€ìˆ˜ ë‚´ë³´ë‚´ê¸°
    if "GITHUB_ENV" in os.environ:
        with open(os.environ["GITHUB_ENV"], "a") as f:
            f.write(f"TOTAL_ROWS={total_rows}\n")

if __name__ == "__main__":
    main()
